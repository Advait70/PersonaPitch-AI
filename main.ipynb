{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwaQq4GRU_Nw"
   },
   "source": [
    "# StarGANv2-VC Demo (VCTK 20 Speakers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCpoXuZeGKAn"
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24923,
     "status": "ok",
     "timestamp": 1613984920200,
     "user": {
      "displayName": "Yinghao Li",
      "photoUrl": "",
      "userId": "12798981472803960591"
     },
     "user_tz": 300
    },
    "id": "3on9IjGhVGTP",
    "outputId": "63a799f8-564d-48c2-fb0f-e66c0cd9fdb8"
   },
   "outputs": [],
   "source": [
    "# load packages\n",
    "import random\n",
    "import yaml\n",
    "from munch import Munch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import librosa\n",
    "\n",
    "from Utils.ASR.models import ASRCNN\n",
    "from Utils.JDC.model import JDCNet\n",
    "from models import Generator, MappingNetwork, StyleEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: http://speech.ee.ntu.edu.tw/~jjery2243542/resource/model/is18/en_speaker_used.txt\n",
    "# Source: https://github.com/jjery2243542/voice_conversion\n",
    "\n",
    "speakers = [225,228,229,230,231,233,236,239,240,244,226,227,232,243,254,256,258,259,270,273]\n",
    "\n",
    "to_mel = torchaudio.transforms.MelSpectrogram(\n",
    "    n_mels=80, n_fft=2048, win_length=1200, hop_length=300)\n",
    "mean, std = -4, 4\n",
    "\n",
    "def preprocess(wave):\n",
    "    wave_tensor = torch.from_numpy(wave).float()\n",
    "    mel_tensor = to_mel(wave_tensor)\n",
    "    mel_tensor = (torch.log(1e-5 + mel_tensor.unsqueeze(0)) - mean) / std\n",
    "    return mel_tensor\n",
    "\n",
    "def build_model(model_params={}):\n",
    "    args = Munch(model_params)\n",
    "    generator = Generator(args.dim_in, args.style_dim, args.max_conv_dim, w_hpf=args.w_hpf, F0_channel=args.F0_channel)\n",
    "    mapping_network = MappingNetwork(args.latent_dim, args.style_dim, args.num_domains, hidden_dim=args.max_conv_dim)\n",
    "    style_encoder = StyleEncoder(args.dim_in, args.style_dim, args.num_domains, args.max_conv_dim)\n",
    "    \n",
    "    nets_ema = Munch(generator=generator,\n",
    "                     mapping_network=mapping_network,\n",
    "                     style_encoder=style_encoder)\n",
    "\n",
    "    return nets_ema\n",
    "\n",
    "def compute_style(speaker_dicts):\n",
    "    reference_embeddings = {}\n",
    "    for key, (path, speaker) in speaker_dicts.items():\n",
    "        if path == \"\":\n",
    "            label = torch.LongTensor([speaker]).to('cuda')\n",
    "            latent_dim = starganv2.mapping_network.shared[0].in_features\n",
    "            ref = starganv2.mapping_network(torch.randn(1, latent_dim).to('cuda'), label)\n",
    "        else:\n",
    "            wave, sr = librosa.load(path, sr=24000)\n",
    "            audio, index = librosa.effects.trim(wave, top_db=30)\n",
    "            if sr != 24000:\n",
    "                wave = librosa.resample(wave, sr, 24000)\n",
    "            mel_tensor = preprocess(wave).to('cuda')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                label = torch.LongTensor([speaker])\n",
    "                ref = starganv2.style_encoder(mel_tensor.unsqueeze(1), label)\n",
    "        reference_embeddings[key] = (ref, label)\n",
    "    \n",
    "    return reference_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load F0 model\n",
    "\n",
    "F0_model = JDCNet(num_class=1, seq_len=192)\n",
    "params = torch.load(\"Utils/JDC/bst.t7\")['net']\n",
    "F0_model.load_state_dict(params)\n",
    "_ = F0_model.eval()\n",
    "F0_model = F0_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 43003,
     "status": "ok",
     "timestamp": 1613984938321,
     "user": {
      "displayName": "Yinghao Li",
      "photoUrl": "",
      "userId": "12798981472803960591"
     },
     "user_tz": 300
    },
    "id": "NZA3ot-oF5t-"
   },
   "outputs": [],
   "source": [
    "# load vocoder\n",
    "import scipy.signal.windows as windows\n",
    "import scipy.signal\n",
    "scipy.signal.kaiser = windows.kaiser\n",
    "\n",
    "from parallel_wavegan.utils import load_model\n",
    "vocoder = load_model(\"Vocoder/checkpoint-400000steps.pkl\").to('cuda').eval()\n",
    "vocoder.remove_weight_norm()\n",
    "_ = vocoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24462,
     "status": "ok",
     "timestamp": 1613985522414,
     "user": {
      "displayName": "Yinghao Li",
      "photoUrl": "",
      "userId": "12798981472803960591"
     },
     "user_tz": 300
    },
    "id": "Ou4367LCyefA",
    "outputId": "19c61f6f-f39a-43b9-9275-09418c2aebb4"
   },
   "outputs": [],
   "source": [
    "# load starganv2\n",
    "\n",
    "model_path = 'Models/epoch_00150.pth'\n",
    "\n",
    "with open('Models/config.yml') as f:\n",
    "    starganv2_config = yaml.safe_load(f)\n",
    "starganv2 = build_model(model_params=starganv2_config[\"model_params\"])\n",
    "params = torch.load(model_path, map_location='cpu')\n",
    "params = params['model_ema']\n",
    "_ = [starganv2[key].load_state_dict(params[key]) for key in starganv2]\n",
    "_ = [starganv2[key].eval() for key in starganv2]\n",
    "starganv2.style_encoder = starganv2.style_encoder.to('cuda')\n",
    "starganv2.mapping_network = starganv2.mapping_network.to('cuda')\n",
    "starganv2.generator = starganv2.generator.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load input wave\n",
    "selected_speakers = [230,258,273]\n",
    "k = random.choice(selected_speakers)\n",
    "wav_path = \"separated/htdemucs/Perfect/vocals.wav\"\n",
    "audio, source_sr = librosa.load(wav_path, sr=24000)\n",
    "audio = audio / np.max(np.abs(audio))\n",
    "audio.dtype = np.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert by style encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with reference, using style encoder\n",
    "\n",
    "speaker_dicts = {}\n",
    "for s in selected_speakers:\n",
    "    k = s\n",
    "    speaker_dicts['p' + str(s)] = (\n",
    "         'Demo/VCTK-corpus/p' + str(k) + '/p' + str(k) + '_023.wav',\n",
    "          speakers.index(s)\n",
    "     )\n",
    "\n",
    "reference_embeddings = compute_style(speaker_dicts)\n",
    "\n",
    "\"\"\"\n",
    "# For a single input audio file in the Demo folder:\n",
    "speaker_dicts = {\n",
    "    'myaudio': (\n",
    "        r\"Demo/VCTK-corpus/p500/p500_023.wav\",  # replace with your WAV path\n",
    "        0  # default speaker label\n",
    "    )\n",
    "}\n",
    "reference_embeddings = compute_style(speaker_dicts)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "executionInfo": {
     "elapsed": 1424,
     "status": "ok",
     "timestamp": 1613986299525,
     "user": {
      "displayName": "Yinghao Li",
      "photoUrl": "",
      "userId": "12798981472803960591"
     },
     "user_tz": 300
    },
    "id": "T5tahObUyN-d",
    "outputId": "f4f38742-2235-4f59-cb2a-5008912cd870",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# conversion \n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "start = time.time()\n",
    "    \n",
    "source = preprocess(audio).to('cuda:0')\n",
    "keys = []\n",
    "converted_samples = {}\n",
    "reconstructed_samples = {}\n",
    "converted_mels = {}\n",
    "\n",
    "# --- Patch: Force vocoder.aux_context_window to a fixed tuple to avoid padding errors ---\n",
    "if hasattr(vocoder, 'aux_context_window'):\n",
    "    # Force it to (4,4); adjust if needed based on your vocoder's requirements.\n",
    "    vocoder.aux_context_window = (4, 4)\n",
    "# ---------------------------------------------------\n",
    "\n",
    "for key, (ref, _) in reference_embeddings.items():\n",
    "    with torch.no_grad():\n",
    "        f0_feat = F0_model.get_feature_GAN(source.unsqueeze(1))\n",
    "        out = starganv2.generator(source.unsqueeze(1), ref, F0=f0_feat)\n",
    "        \n",
    "        # 'out' is the full mel spectrogram from the generator.\n",
    "        # Transpose and squeeze to get shape [T, n_mels]\n",
    "        c = out.transpose(-1, -2).squeeze().to('cuda')\n",
    "        \n",
    "        # Determine the minimum required chunk length from the vocoder's auxiliary context.\n",
    "        aux_context_window = vocoder.aux_context_window  # now a tuple, e.g., (4, 4)\n",
    "        min_chunk_length = aux_context_window[0]\n",
    "        \n",
    "        # Set chunk size: ensure it's at least 1000 frames or the minimum required.\n",
    "        chunk_size = max(1000, min_chunk_length)\n",
    "        print(f\"Using chunk size: {chunk_size}, min required: {min_chunk_length}\")\n",
    "        \n",
    "        converted_audio_chunks = []\n",
    "        for i in range(0, c.shape[0], chunk_size):\n",
    "            chunk = c[i:i+chunk_size]  # shape: [chunk_frames, n_mels]\n",
    "            # If the chunk's time dimension is too short, pad it by repeating the last frame.\n",
    "            if chunk.shape[0] < min_chunk_length:\n",
    "                pad_amount = min_chunk_length - chunk.shape[0]\n",
    "                last_frame = chunk[-1:].clone()  # shape: [1, n_mels]\n",
    "                pad_tensor = last_frame.repeat(pad_amount, 1)\n",
    "                chunk = torch.cat([chunk, pad_tensor], dim=0)\n",
    "            # Reshape to [1, n_mels, T] as expected by the vocoder\n",
    "            chunk = chunk.unsqueeze(0).transpose(1, 2)\n",
    "            print(f\"Processing chunk with shape: {chunk.shape}\")\n",
    "            # Get expected length via upsample_net\n",
    "            with torch.no_grad():\n",
    "                c_up = vocoder.upsample_net(chunk)\n",
    "            expected_T = c_up.shape[-1]\n",
    "            # Create dummy noise with shape [1, 1, expected_T]\n",
    "            dummy_noise = torch.randn(1, 1, expected_T).to(chunk.device)\n",
    "            with torch.no_grad():\n",
    "                y_chunk = vocoder.forward(dummy_noise, chunk)\n",
    "            # Flatten the output and move to CPU\n",
    "            y_chunk = y_chunk.squeeze(0).transpose(1, 0).contiguous().view(-1).cpu().numpy()\n",
    "            converted_audio_chunks.append(y_chunk)\n",
    "        \n",
    "        # Concatenate all chunks into one waveform\n",
    "        y_out = np.concatenate(converted_audio_chunks)\n",
    "        \n",
    "        # --- Reconstruction for Reference ---\n",
    "        if key not in speaker_dicts or speaker_dicts[key][0] == \"\":\n",
    "            recon = None\n",
    "        else:\n",
    "            wave, sr = librosa.load(speaker_dicts[key][0], sr=24000)\n",
    "            mel = preprocess(wave)\n",
    "            c_recon = mel.transpose(-1, -2).squeeze().to('cuda')\n",
    "            if c_recon.shape[0] < min_chunk_length:\n",
    "                pad_amount = min_chunk_length - c_recon.shape[0]\n",
    "                last_frame = c_recon[-1:].clone()\n",
    "                pad_tensor = last_frame.repeat(pad_amount, 1)\n",
    "                c_recon = torch.cat([c_recon, pad_tensor], dim=0)\n",
    "            c_recon = c_recon.unsqueeze(0).transpose(1, 2)\n",
    "            # Determine expected time dimension for reconstruction\n",
    "            with torch.no_grad():\n",
    "                c_up_recon = vocoder.upsample_net(c_recon)\n",
    "            expected_T_recon = c_up_recon.shape[-1]\n",
    "            dummy_noise_recon = torch.randn(1, 1, expected_T_recon).to(c_recon.device)\n",
    "            with torch.no_grad():\n",
    "                recon = vocoder.forward(dummy_noise_recon, c_recon)\n",
    "            recon = recon.squeeze(0).transpose(1, 0).contiguous().view(-1).cpu().numpy()\n",
    "\n",
    "    converted_samples[key] = y_out\n",
    "    reconstructed_samples[key] = recon\n",
    "    converted_mels[key] = out    \n",
    "    keys.append(key)\n",
    "\n",
    "end = time.time()\n",
    "print('total processing time: %.3f sec' % (end - start) )\n",
    "\n",
    "for key, wave in converted_samples.items():\n",
    "    print('Converted: %s' % key)\n",
    "    ipd.display(ipd.Audio(wave, rate=24000))\n",
    "    print('Reference (vocoder): %s' % key)\n",
    "    if reconstructed_samples[key] is not None:\n",
    "        ipd.display(ipd.Audio(reconstructed_samples[key], rate=24000))\n",
    "\"\"\"\n",
    "print('Original (vocoder):')\n",
    "wave, sr = librosa.load(wav_path, sr=24000)\n",
    "mel = preprocess(wave)\n",
    "c_orig = mel.transpose(-1, -2).squeeze().to('cuda')\n",
    "with torch.no_grad():\n",
    "    c_orig = c_orig.unsqueeze(0).transpose(1, 2)\n",
    "    if c_orig.shape[2] < min_chunk_length:\n",
    "        pad_amount = min_chunk_length - c_orig.shape[2]\n",
    "        last_frame_orig = c_orig[:, :, -1:].clone()\n",
    "        pad_tensor_orig = last_frame_orig.repeat(1, 1, pad_amount)\n",
    "        c_orig = torch.cat([c_orig, pad_tensor_orig], dim=2)\n",
    "    recon = vocoder.inference(c_orig)\n",
    "    recon = recon.view(-1).cpu().numpy()\n",
    "ipd.display(ipd.Audio(recon, rate=24000))\n",
    "print('Original:')\n",
    "ipd.display(ipd.Audio(wav_path, rate=24000))\n",
    "\"\"\"\n",
    "print('Original (raw stem):')\n",
    "ipd.display(ipd.Audio(wav_path, rate=24000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWh3o9hvGvJt"
   },
   "source": [
    "#### Convert by mapping network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Vocal Cleanup, EQ & Instrumental Mix in One Cell ---\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import scipy.signal as sps\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "sr = 24000\n",
    "\n",
    "# 1) Pull out your converted waveform\n",
    "if 'converted_samples' not in globals():\n",
    "    raise NameError(\"`converted_samples` not found—run your conversion cell first.\")\n",
    "# take the first (or only) entry\n",
    "converted_audio = list(converted_samples.values())[0]\n",
    "print(\"Using converted_samples key:\", list(converted_samples.keys())[0])\n",
    "\n",
    "# 2) Noise reduction (optional, requires noisereduce)\n",
    "try:\n",
    "    import noisereduce as nr\n",
    "    noise_clip     = converted_audio[:sr]  # first second as noise profile\n",
    "    denoised_vocals = nr.reduce_noise(\n",
    "    y=converted_audio,                 # your converted waveform\n",
    "    y_noise=converted_audio[:sr],      # first second as noise profile\n",
    "    sr=sr,\n",
    "    stationary=False,\n",
    "    prop_decrease=0.8,                 # try 0.4–0.8; lower = gentler\n",
    "    time_mask_smooth_ms=200,           # try 50–200 ms\n",
    "    freq_mask_smooth_hz=500            # try 50–500 Hz\n",
    ")\n",
    "\n",
    "    print(\"✅ Noise reduction applied\")\n",
    "except ImportError:\n",
    "    print(\"⚠ noisereduce not installed; skipping noise reduction\")\n",
    "    denoised_vocals = converted_audio\n",
    "\n",
    "# 3) High‑pass filter at 80 Hz to remove low rumble\n",
    "b, a        = sps.butter(4, 80/(sr/2), btype='highpass')\n",
    "eq_vocals   = sps.filtfilt(b, a, denoised_vocals)\n",
    "print(\"✅ High‑pass EQ at 80 Hz applied\")\n",
    "\n",
    "# quick listen to cleaned vocals\n",
    "display(Audio(eq_vocals, rate=sr))\n",
    "\n",
    "# … previous steps remain unchanged up through loading/pre‑EQ …\n",
    "\n",
    "# 4) Load instrumental & align lengths\n",
    "inst_path = \"separated/htdemucs/Perfect/no_vocals.wav\"\n",
    "inst_audio, _ = librosa.load(inst_path, sr=sr)\n",
    "min_len       = min(len(eq_vocals), len(inst_audio))\n",
    "vocals_trim   = eq_vocals[:min_len]\n",
    "inst_trim     = inst_audio[:min_len]\n",
    "\n",
    "# 5) Normalize & mix at a true 50/50 ratio\n",
    "vocals_norm = vocals_trim / np.max(np.abs(vocals_trim))\n",
    "inst_norm   = inst_trim   / np.max(np.abs(inst_trim))\n",
    "mixed       = 1.2 * vocals_norm + 0.5 * inst_norm\n",
    "mixed       = mixed / np.max(np.abs(mixed)) * 0.95\n",
    "\n",
    "# 6) Save & play\n",
    "sf.write(\"final_song_mix.wav\", mixed.astype(np.float32), sr)\n",
    "display(Audio(mixed, rate=sr))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "hCpoXuZeGKAn"
   ],
   "name": "Starganv2_vc.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
